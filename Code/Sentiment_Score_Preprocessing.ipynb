{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e46f4d",
   "metadata": {},
   "source": [
    "1. Import all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "019237ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "404ced92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/eric_p/miniconda3/envs/eric_p/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"yiyanghkust/finbert-tone\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f37d4dd",
   "metadata": {},
   "source": [
    "2. Import the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b783629",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = \"/Users/eric_p/Desktop/Fall 2025/MFIN 7036/Group Project/Text_Data/BTC_match_text.csv\"\n",
    "source_data = pd.read_csv(input_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f93a78",
   "metadata": {},
   "source": [
    "3. Extract the columns that will be passed into FinBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea62cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = source_data[[\"date_time\", \"title\", \"article_text\"]].copy()\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "df[\"date_time\"] = pd.to_datetime(df[\"date_time\"])\n",
    "df[\"date\"] = df[\"date_time\"].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e8dab",
   "metadata": {},
   "source": [
    "4. Clean the text extracted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae32f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(df, str):\n",
    "        return \"\"\n",
    "\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove numbered lists at beginning of lines\n",
    "    text = re.sub(r\"\\n?\\d+\\.\\s+.*\", \"\", text)\n",
    "\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69596f",
   "metadata": {},
   "source": [
    "4. Truncate text tokens from the end of the text (with a max limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46219fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_from_end(text, tokenizer, max_tokens=400):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return text\n",
    "    tokens = tokens[-max_tokens:]\n",
    "    return tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41b44b6",
   "metadata": {},
   "source": [
    "5. Construct FinBert input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce583dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99400/99400 [00:03<00:00, 31222.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_finbert_text(row, tokenizer):\n",
    "    title = row[\"title\"] if isinstance(row[\"title\"], str) else \"\"\n",
    "    body = clean_text(row[\"article_text\"])\n",
    "    body = truncate_from_end(body, tokenizer, max_tokens=400)\n",
    "    return title + \" [SEP] \" + body\n",
    "\n",
    "df[\"finbert_text\"] = df.progress_apply(\n",
    "    lambda x: build_finbert_text(x, tokenizer),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13e2df",
   "metadata": {},
   "source": [
    "6. Store FinBert input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"/Users/eric_p/Desktop/Fall 2025/MFIN 7036/Group Project/Text_Data/Finbert_input/finbert_input.csv\"\n",
    "df.to_csv(output_file_path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eric_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
