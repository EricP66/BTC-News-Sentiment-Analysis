{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f929c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #For regular expression searching\n",
    "import os\n",
    "import string \n",
    "from bs4 import BeautifulSoup #For Deleting HTML tag\n",
    "import emoji \n",
    "import pandas as pd\n",
    "import nltk #For Tokenization and Lemmatization\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy #For More accurate Lemmatization\n",
    "from tqdm.notebook import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87c5ca33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Several necessary packages\n",
    "#nltk\n",
    "try:\n",
    "    nltk.download('punkt',quiet=True) #for word_toknize,quiet mode\n",
    "    nltk.download('stopwords',quiet=True) #Download dictionary of stopwords\n",
    "    nltk.download('wordnet',quiet=True) #Dataset of word lemmatization \n",
    "except Exception as e:\n",
    "    print(f'Wrong for downloading:{e}')\n",
    "\n",
    "#Spacy Faster and more accurate lemmatization\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm',disable=[\"parser\",\"ner\"]) #Don't need to download parser and ner\n",
    "except:\n",
    "    nlp = None\n",
    "    \n",
    "stop_words = set(stopwords.words('english')) #Transfer list to set, O(n) --> O(1)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07e4f738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x16c4653d0>\n"
     ]
    }
   ],
   "source": [
    "print(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0d38e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clean(text):\n",
    "    \"\"\"Pre-clean function: \n",
    "    like removing HTML tag; Emoji and special punctuation\"\"\"\n",
    "    if not isinstance(text,str) or not text.strip(): #if text is str format or only whitespace\n",
    "        return \"\"\n",
    "    # remove html tag\n",
    "    text = BeautifulSoup(text,'html.parser').get_text() \n",
    "    \n",
    "    # remove emoji to null string\n",
    "    text = emoji.replace_emoji(text,replace = '')\n",
    "    \n",
    "    #remove special character string\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"-]', '', text)\n",
    "    #we define legal character,any character not in list will be removed\n",
    "    \n",
    "    #remove several whitespace/tab to single whitespace\n",
    "    text = re.sub(r'\\s+',' ',text.strip())\n",
    "    \n",
    "    #remove several whitespace/tab before puncuation\n",
    "    text = re.sub(r'\\s+([.,!?])',r'\\1',text)\n",
    "    text = re.sub(r'\\s+',' ',text.strip())\n",
    "    \n",
    "    text = re.sub(r'www.\\w+.com','',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38f8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any student! \n"
     ]
    }
   ],
   "source": [
    "str_example = 'any %&((() üòÅ))   student $@***! www.example.com'\n",
    "print(pre_clean(str_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_english_text(\n",
    "#     text,\n",
    "#     batch_size = 500, #less memory\n",
    "#     n_process  = 4,\n",
    "#     use_spacy_lemmatize:bool = True,\n",
    "#     remove_stopwords:bool = True,\n",
    "#     show_progress = True): #return iteration\n",
    "#     \"\"\"Clean English text and return both cleaned result and statistics\n",
    "#     \"\"\"\n",
    "#     if isinstance(text,str):\n",
    "#         text = [text]\n",
    "        \n",
    "#     if nlp is None:print('NLP no download')\n",
    "#     # Use nlp.pipe() for batch processing\n",
    "#     #nlp.pipe() us designed for large-scale text processing\n",
    "#     pipe_iterator = nlp.pipe(\n",
    "#         text,\n",
    "#         batch_size=batch_size,\n",
    "#         n_process = n_process,\n",
    "#         disable = ['parser','ner'] if use_spacy_lemmatize else ['tagger','parser','ner','lemmatizer']\n",
    "#     )\n",
    "    \n",
    "#     #show progress\n",
    "#     iterator = tqdm(pipe_iterator,total = len(text),desc = \"Cleaning(pipe)\",unit=\"text\") \\\n",
    "#     if show_progress else pipe_iterator\n",
    "    \n",
    "#     total_stats = {\n",
    "#     \"total_original_chars\": 0,\n",
    "#     \"total_cleaned_chars\": 0,\n",
    "#     \"total_removed_chars\": 0,\n",
    "#     \"total_original_words\": 0,\n",
    "#     \"total_cleaned_words\": 0,\n",
    "#     \"removed_ratio\": 0.0}\n",
    "    \n",
    "#     #main iteration\n",
    "#     for doc in iterator:\n",
    "#         original_text = doc.text\n",
    "#         original_len = len(doc.text)\n",
    "#         original_words = len(original_text.split())\n",
    "        \n",
    "#         if use_spacy_lemmatize:    #Token lemmatization\n",
    "#             tokens = [\n",
    "#                 token.lemma_\n",
    "#                 for token in doc  #nlp.pipe() will tokenize the doc automatically\n",
    "#                 if not token.is_punct\n",
    "#                 and not token.is_space\n",
    "#                 and token.text.strip()\n",
    "#             ]\n",
    "#         else:\n",
    "#             tokens = word_tokenize(original_text.lower())\n",
    "#             tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "#         #remove all stopwards\n",
    "#         if remove_stopwords:\n",
    "#             tokens = [t for t in tokens if t not in stop_words and len(t)>1]\n",
    "#         cleaned = ' '.join(tokens)      #recombine all tokens\n",
    "#         cleaned_len = len(cleaned)\n",
    "#         cleaned_words = len(tokens)\n",
    "        \n",
    "#         stats = {'removed_chars':original_len - cleaned_len,\n",
    "#                  \"remove_ratio\":round((original_len-cleaned_len)/original_len,2)\\\n",
    "#                  if original_len >0 else 0.0}\n",
    "        \n",
    "#         #sum of all cleaned\n",
    "#         total_stats[\"total_original_chars\"] += original_len\n",
    "#         total_stats[\"total_cleaned_chars\"] += cleaned_len\n",
    "#         total_stats[\"total_removed_chars\"] += stats[\"removed_chars\"]\n",
    "#         total_stats[\"total_original_words\"] += original_words\n",
    "#         total_stats[\"total_cleaned_words\"] += cleaned_words\n",
    "        \n",
    "#         yield cleaned\n",
    "\n",
    "#     if total_stats[\"total_original_chars\"] > 0:\n",
    "#         total_stats[\"removed_ratio\"] = round(\n",
    "#             total_stats[\"total_removed_chars\"] / total_stats[\"total_original_chars\"] * 100,\n",
    "#             1\n",
    "#         )\n",
    "#     print(\"\\n\" + \"‚ïê\" * 60)\n",
    "#     print(\"Text Cleaning Summary (Pipe Mode):\")\n",
    "#         # Handle case where texts is a generator (no len())\n",
    "#     print(f\"Total texts processed : {len(text):,}\" if hasattr(text, '__len__') else \"Unknown (streaming)\")\n",
    "#     print(f\"Total original characters : {total_stats['total_original_chars']:,}\")\n",
    "#     print(f\"Total cleaned characters : {total_stats['total_cleaned_chars']:,}\")\n",
    "#     print(f\"Total characters removed : {total_stats['total_removed_chars']:,} ({total_stats['removed_ratio']}%)\")\n",
    "#     print(f\"Total original words : {total_stats['total_original_words']:,}\")\n",
    "#     print(f\"Total cleaned words : {total_stats['total_cleaned_words']:,}\")\n",
    "#     print(\"‚ïê\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8e34e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_english_text(\n",
    "    text,\n",
    "    batch_size = 500, #less memory\n",
    "    n_process  = 4,\n",
    "    use_spacy_lemmatize:bool = False,\n",
    "    remove_stopwords:bool = True,\n",
    "    show_progress = True): #return iteration\n",
    "    \"\"\"Clean English text and return both cleaned result and statistics\n",
    "    Use nltk because efficency\n",
    "    \"\"\"\n",
    "    if isinstance(text,str):\n",
    "        text = [text]\n",
    "        \n",
    "    if nlp is None:print('NLP no download')\n",
    "    \n",
    "    #show progress\n",
    "    iterator = tqdm(text,total = len(text),desc = \"Cleaning\",unit=\"text\") \\\n",
    "    if show_progress else text\n",
    "    \n",
    "    total_stats = {\n",
    "    \"total_original_chars\": 0,\n",
    "    \"total_cleaned_chars\": 0,\n",
    "    \"total_removed_chars\": 0,\n",
    "    \"total_original_words\": 0,\n",
    "    \"total_cleaned_words\": 0,\n",
    "    \"removed_ratio\": 0.0}\n",
    "    \n",
    "    #main iteration\n",
    "    for original_text in iterator:\n",
    "        original_len = len(original_text)\n",
    "        original_words = len(original_text.split())\n",
    "        \n",
    "        #Word Lemmatization\n",
    "        tokens = word_tokenize(original_text.lower())\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "        \n",
    "        #remove all stopwards\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in stop_words and len(t)>1]\n",
    "        cleaned = ' '.join(tokens)      #recombine all tokens\n",
    "        cleaned_len = len(cleaned)\n",
    "        cleaned_words = len(tokens)\n",
    "        \n",
    "        stats = {'removed_chars':original_len - cleaned_len,\n",
    "                 \"remove_ratio\":round((original_len-cleaned_len)/original_len,2)\\\n",
    "                 if original_len >0 else 0.0}\n",
    "        \n",
    "        #sum of all cleaned\n",
    "        total_stats[\"total_original_chars\"] += original_len\n",
    "        total_stats[\"total_cleaned_chars\"] += cleaned_len\n",
    "        total_stats[\"total_removed_chars\"] += stats[\"removed_chars\"]\n",
    "        total_stats[\"total_original_words\"] += original_words\n",
    "        total_stats[\"total_cleaned_words\"] += cleaned_words\n",
    "        \n",
    "        yield cleaned\n",
    "\n",
    "    if total_stats[\"total_original_chars\"] > 0:\n",
    "        total_stats[\"removed_ratio\"] = round(\n",
    "            total_stats[\"total_removed_chars\"] / total_stats[\"total_original_chars\"] * 100,\n",
    "            1\n",
    "        )\n",
    "    print(\"\\n\" + \"‚ïê\" * 60)\n",
    "    print(\"Text Cleaning Summary (Pipe Mode):\")\n",
    "        # Handle case where texts is a generator (no len())\n",
    "    print(f\"Total texts processed : {len(text):,}\" if hasattr(text, '__len__') else \"Unknown (streaming)\")\n",
    "    print(f\"Total original characters : {total_stats['total_original_chars']:,}\")\n",
    "    print(f\"Total cleaned characters : {total_stats['total_cleaned_chars']:,}\")\n",
    "    print(f\"Total characters removed : {total_stats['total_removed_chars']:,} ({total_stats['removed_ratio']}%)\")\n",
    "    print(f\"Total original words : {total_stats['total_original_words']:,}\")\n",
    "    print(f\"Total cleaned words : {total_stats['total_cleaned_words']:,}\")\n",
    "    print(\"‚ïê\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "093e9721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9970bd9e04094d718251de9302407fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/8 [00:00<?, ?text/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check new video httpsyoutu.bedqw4w9wgxcq user123\n",
      "quick brown fox jump lazy dog\n",
      "hello world visit info\n",
      "ca n't believe 's already 2026 ... unbelievable\n",
      "long sentence lot test stopwords removal\n",
      "\n",
      "\n",
      "python awesome python great python python python\n",
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 8\n",
      "Total original characters : 339\n",
      "Total cleaned characters : 234\n",
      "Total characters removed : 105 (31.0%)\n",
      "Total original words : 57\n",
      "Total cleaned words : 36\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"Check out my new video! https://youtu.be/dQw4w9WgXcQ üòéüî• @user123\",\n",
    "    \"The quick brown fox jumps over the lazy dog!!!\",\n",
    "    \"<p>Hello <b>world</b>!!!</p> Visit www.example.com for more info.\",\n",
    "    \"I can't believe it's already 2026... unbelievable!!! ‚ù§Ô∏èüöÄ\",\n",
    "    \"This is a very very long sentence with lots of the and is and are to test stopwords removal.\",\n",
    "    \"\",  \n",
    "    \"   \\n\\t   \",  \n",
    "    \"Python is awesome! Python is great! Python Python Python.\"  \n",
    "]\n",
    "test_texts = [pre_clean(i) for i in test_texts]\n",
    "gen = clean_english_text(test_texts,n_process=1)\n",
    "for exp_text in gen:\n",
    "    print(exp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d7d7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/fusiyuan/Desktop/HKU Courses/Text Analytics and Natural Language Processing/MFIN7036 Code and Data-20251210/my code/Group work/bitcoin_comments_sorted.parquet'\n",
    "df = pd.read_parquet(file_path,columns=['datetime','body','score'], engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d963065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90029d3994664e6fba6a93cdedabbf0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589433e7c55444728cdddd9f84dd2afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5d9da8844c463bb46195557079c6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cleaning:   0%|          | 0/100 [00:00<?, ?text/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100\n",
      "Total original characters : 55,483\n",
      "Total cleaned characters : 37,394\n",
      "Total characters removed : 18,089 (32.6%)\n",
      "Total original words : 9,413\n",
      "Total cleaned words : 5,221\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "                                                body  \\\n",
      "0  Interesting, it uses IRC as a high level proto...   \n",
      "1  No - the richest person will be the one with t...   \n",
      "2  &gt;a public list of all the previous transact...   \n",
      "3  No, that's not how bitcoin works, check out th...   \n",
      "4  It's weird how Ron Paul gets money so incredib...   \n",
      "5  Some reddit thought on Bitcoin [here](http://w...   \n",
      "6  Scroll down more, bud.-\\n\\nSo your wealth is d...   \n",
      "7  I just found BitCoin while looking for Crypto ...   \n",
      "8  No, most emphatically nonsentient systems. Bas...   \n",
      "9  Is bitcoin closed source? It Looks more like a...   \n",
      "\n",
      "                                         pre_cleaned  \\\n",
      "0  Interesting, it uses IRC as a high level proto...   \n",
      "1  No - the richest person will be the one with t...   \n",
      "2  a public list of all the previous transactions...   \n",
      "3  No, that's not how bitcoin works, check out th...   \n",
      "4  It's weird how Ron Paul gets money so incredib...   \n",
      "5  Some reddit thought on Bitcoin herehttprbusine...   \n",
      "6  Scroll down more, bud.- So your wealth is dete...   \n",
      "7  I just found BitCoin while looking for Crypto ...   \n",
      "8  No, most emphatically nonsentient systems. Bas...   \n",
      "9  Is bitcoin closed source? It Looks more like a...   \n",
      "\n",
      "                                        Cleaned_body  \n",
      "0  interesting us irc high level protocol see bit...  \n",
      "1  richest person one computing power 1.4 new bit...  \n",
      "2  public list previous transaction collectively ...  \n",
      "3  's bitcoin work check paper httpsuperb-west.dl...  \n",
      "4  's weird ron paul get money incredibly wrong w...  \n",
      "5  reddit thought bitcoin herehttprbusinesscommen...  \n",
      "6  scroll bud.- wealth determined amount cpu powe...  \n",
      "7  found bitcoin looking crypto project think 'm ...  \n",
      "8  emphatically nonsentient system basically some...  \n",
      "9             bitcoin closed source look like bitcon  \n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "df_sample = df.head(100).copy()\n",
    "df_sample['pre_cleaned'] = df_sample['body'].progress_apply(pre_clean)\n",
    "pre_cleaned_list = list(df_sample['pre_cleaned'])\n",
    "gen = clean_english_text(pre_cleaned_list,n_process=1,show_progress = True)\n",
    "cleaned_results = []\n",
    "for term in tqdm(gen,total = len(pre_cleaned_list),desc = 'Collecting Result'):    #End = len(pre_cleaned_list)\n",
    "    cleaned_results.append(term)\n",
    "\n",
    "df_sample['Cleaned_body'] = cleaned_results\n",
    "\n",
    "print(df_sample[['body','pre_cleaned','Cleaned_body']][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b743b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fce54b6af764abf9999f640fae6a9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Progress(Chunks):   0%|          | 0/79 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf360b81177495ead0124769ea1d6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a058b2688c5e4100ada5bde659ee2c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 48,375,742\n",
      "Total cleaned characters : 32,005,042\n",
      "Total characters removed : 16,370,700 (33.8%)\n",
      "Total original words : 8,403,265\n",
      "Total cleaned words : 4,621,360\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:0 - 100000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba1860552284679a0f92312cc131c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b6719c9a994630bd43e01186883a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 45,892,662\n",
      "Total cleaned characters : 30,328,418\n",
      "Total characters removed : 15,564,244 (33.9%)\n",
      "Total original words : 7,958,793\n",
      "Total cleaned words : 4,378,374\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:100000 - 200000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e775a93851514d0a91e9d302f858ce68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8750e0474fc4aa6902f0e374e825f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 43,147,762\n",
      "Total cleaned characters : 28,352,214\n",
      "Total characters removed : 14,795,548 (34.3%)\n",
      "Total original words : 7,549,764\n",
      "Total cleaned words : 4,133,887\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:200000 - 300000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c152ceddc1487f9e4468661d54f32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "917b3a3befbf4b7ca48761e4b4c09546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 43,431,847\n",
      "Total cleaned characters : 28,622,801\n",
      "Total characters removed : 14,809,046 (34.1%)\n",
      "Total original words : 7,572,709\n",
      "Total cleaned words : 4,161,768\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:300000 - 400000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c7d3d2ee924c7facc525182c53bbdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2456dda4feb4a00ab17810e78dac673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 44,785,243\n",
      "Total cleaned characters : 29,532,523\n",
      "Total characters removed : 15,252,720 (34.1%)\n",
      "Total original words : 7,790,598\n",
      "Total cleaned words : 4,290,219\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:400000 - 500000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6192cabff5ed4c73afac736a12260929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5096781ae9d04aa0a37f7cfb7ff5a044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 41,687,207\n",
      "Total cleaned characters : 27,629,289\n",
      "Total characters removed : 14,057,918 (33.7%)\n",
      "Total original words : 7,229,670\n",
      "Total cleaned words : 4,006,002\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:500000 - 600000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5940da4fce00417ba2c07f8c0e2b454f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3efd694ec2ce4d1bb73591c5e8964f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 43,257,497\n",
      "Total cleaned characters : 28,829,279\n",
      "Total characters removed : 14,428,218 (33.4%)\n",
      "Total original words : 7,449,525\n",
      "Total cleaned words : 4,141,546\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:600000 - 700000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0494f514a4b845b989e6b63ac21dfb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f508e8d490984859a2417b4f6fc2bf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 35,073,693\n",
      "Total cleaned characters : 24,264,856\n",
      "Total characters removed : 10,808,837 (30.8%)\n",
      "Total original words : 5,787,242\n",
      "Total cleaned words : 3,337,598\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:700000 - 800000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25cd68b24d0448889a3e227b07eefab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba5aa4006334ce1a499997b6e0b60c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 37,024,217\n",
      "Total cleaned characters : 25,440,756\n",
      "Total characters removed : 11,583,461 (31.3%)\n",
      "Total original words : 6,161,856\n",
      "Total cleaned words : 3,528,900\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:800000 - 900000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e58897aa94432d99fbce391d41c98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd78a143f0784b159f4944a1dbf91779",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 40,201,693\n",
      "Total cleaned characters : 27,135,009\n",
      "Total characters removed : 13,066,684 (32.5%)\n",
      "Total original words : 6,853,356\n",
      "Total cleaned words : 3,866,256\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:900000 - 1000000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9078f513f48d40e1b043b6fb089a79fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2921d23c78574befb169d2bcb84c1bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 43,997,450\n",
      "Total cleaned characters : 29,625,858\n",
      "Total characters removed : 14,371,592 (32.7%)\n",
      "Total original words : 7,517,443\n",
      "Total cleaned words : 4,222,857\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:1000000 - 1100000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f861254e789744b39702acd4ac4d3f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473dbc6840c646208a86a531a858114c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "Text Cleaning Summary (Pipe Mode):\n",
      "Total texts processed : 100,000\n",
      "Total original characters : 54,410,009\n",
      "Total cleaned characters : 37,367,973\n",
      "Total characters removed : 17,042,036 (31.3%)\n",
      "Total original words : 9,037,400\n",
      "Total cleaned words : 5,160,152\n",
      "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
      "\n",
      "This chunk finish:1100000 - 1200000 rows;\n",
      "Pre cleaned\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387ba8489ce0449080d1cf2fb812ef93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6346ab3e9a284cc7a3657b98a735f4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting Result:   0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m gen \u001b[38;5;241m=\u001b[39m clean_english_text(pre_cleaned_list,show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     27\u001b[0m cleaned_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m tqdm(gen,total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pre_cleaned_list),desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollecting Result\u001b[39m\u001b[38;5;124m'\u001b[39m,dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     29\u001b[0m     cleaned_results\u001b[38;5;241m.\u001b[39mappend(term)\u001b[38;5;66;03m#End = len(pre_cleaned_list)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df_output_chunk \u001b[38;5;241m=\u001b[39m df_chunk[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/Desktop/HKU Courses/.venv/lib/python3.9/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/HKU Courses/.venv/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[52], line 34\u001b[0m, in \u001b[0;36mclean_english_text\u001b[0;34m(text, batch_size, n_process, use_spacy_lemmatize, remove_stopwords, show_progress)\u001b[0m\n\u001b[1;32m     31\u001b[0m original_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(original_text\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#Word Lemmatization\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#remove all stopwards\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/HKU Courses/.venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:143\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/HKU Courses/.venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/HKU Courses/.venv/lib/python3.9/site-packages/nltk/tokenize/destructive.py:182\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    179\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS2:\n\u001b[0;32m--> 182\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m1 \u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m2 \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCONTRACTIONS3:\n\u001b[1;32m    184\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m1 \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m2 \u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from bs4 import MarkupResemblesLocatorWarning\n",
    "warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "\n",
    "chunk_size = 100000\n",
    "final_out_put_path = '/Users/fusiyuan/Desktop/HKU Courses/bitcoin_comments_sorted_cleaned_full.parquet'\n",
    "output_dir = '/Users/fusiyuan/Desktop/HKU Courses/cleaned_chunks/'\n",
    "os.makedirs(output_dir, exist_ok=True)  \n",
    "total_rows = len(df)\n",
    "total_chunks = (total_rows + chunk_size - 1)//chunk_size\n",
    "for chunk_idx in tqdm(range(total_chunks),desc= 'Overall Progress(Chunks)',unit = 'chunk'):\n",
    "    start = chunk_idx * chunk_size\n",
    "    end = min(start + chunk_size,total_rows)\n",
    "    chunk_path = f\"{output_dir}chunk_{chunk_idx+1:03d}.parquet\"\n",
    "    \n",
    "    print('Pre cleaned')\n",
    "    df_chunk = df.iloc[start:end].copy()\n",
    "    df_chunk['pre_cleaned'] = df_chunk['body'].progress_apply(pre_clean)\n",
    "    \n",
    "    pre_cleaned_list = df_chunk['pre_cleaned'].tolist()\n",
    "    \n",
    "    print('Spacy clean')\n",
    "    gen = clean_english_text(pre_cleaned_list,show_progress=False)\n",
    "    cleaned_results = []\n",
    "    for term in tqdm(gen,total = len(pre_cleaned_list),desc = 'Collecting Result',dynamic_ncols=False):\n",
    "        cleaned_results.append(term)#End = len(pre_cleaned_list)\n",
    "    df_output_chunk = df_chunk[['datetime', 'score']].copy()\n",
    "    df_output_chunk['cleaned_body'] = cleaned_results\n",
    "    \n",
    "    df_output_chunk.to_parquet(chunk_path,engine='pyarrow',index = False)\n",
    "    print(f\"This chunk finish:{start} - {end} rows;\")\n",
    "    \n",
    "    del df_chunk, pre_cleaned_list, cleaned_results, df_output_chunk\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "652e5e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp ÊòØÂê¶Âä†ËΩΩÊàêÂäüÔºü True\n",
      "nlp Á±ªÂûã: <class 'spacy.lang.en.English'>\n"
     ]
    }
   ],
   "source": [
    "print(\"nlp ÊòØÂê¶Âä†ËΩΩÊàêÂäüÔºü\", nlp is not None)\n",
    "print(\"nlp Á±ªÂûã:\", type(nlp) if nlp is not None else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5386dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_clean_vader(text):\n",
    "    \"\"\"Pre-clean function: \n",
    "    like removing HTML tag; Emoji and special punctuation\"\"\"\n",
    "    if not isinstance(text,str) or not text.strip(): #if text is str format or only whitespace\n",
    "        return \"\"\n",
    "    # remove html tag\n",
    "    text = BeautifulSoup(text,'html.parser').get_text() \n",
    "    \n",
    "    #vader could identify the emoji\n",
    "    # remove emoji to null string \n",
    "    # text = emoji.replace_emoji(text,replace = '')\n",
    "    \n",
    "    #remove special character string\n",
    "    text = re.sub(r'[^\\w\\s.,!?\\'\"-]', '', text)\n",
    "    #we define legal character,any character not in list will be removed\n",
    "    \n",
    "    #remove several whitespace/tab to single whitespace\n",
    "    text = re.sub(r'\\s+',' ',text.strip())\n",
    "    \n",
    "    #remove several whitespace/tab before puncuation\n",
    "    text = re.sub(r'\\s+([.,!?])',r'\\1',text)\n",
    "    text = re.sub(r'\\s+',' ',text.strip())\n",
    "    \n",
    "    text = re.sub(r'www.\\w+.com','',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9364f1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b1eb09b7d6f4e04b01e3d93a55ef371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7862669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "output_file_path_vader = '/Users/fusiyuan/Desktop/HKU Courses/Text Analytics and Natural Language Processing/MFIN7036 Code and Data-20251210/my code/data_cleaned_vader.parquet'\n",
    "df_sample = df.copy()\n",
    "df_sample['cleaned_body'] = df_sample['body'].progress_apply(pre_clean_vader)\n",
    "df_clean_vader = df_sample[['datetime','cleaned_body','score']]\n",
    "df_clean_vader.to_parquet(output_file_path_vader,engine= 'pyarrow',index = False)\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
